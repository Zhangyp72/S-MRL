# Overview of S-MRL
  In multimodal representation learning, different modalities contain different amounts of information and show different advantages, leading to modal collapse in cross-modal information integration. In order to solve this problem, we propose a method for multimodal representation learning in a spiking neural networks framework(S-MRL). S-MRL transforms the traditional multimodal learning into a process of spike sequence learning by encoding individual spikes of modalities, thereby minimizing the inter-modal interference. In the learning stage, the multimodal representation of the imbalanced information of different modes in the shared space leads to a modal that may rely on only one or a few modes to make decisions, reducing the overall performance and generalization ability. We use Gaussian kernel spike distance to construct the shared information space between modalities, and then use joint entropy to capture cross-modal information interactions so as to integrate information between different modalities, and construct recursive spiking neural networks to solve modal collapse and forgetting during the learning process.
	In the inference stage, S-MRL utilizes an informative fusion mechanism based on modal data representations to integrate multimodal information, and uses cross-entropy to predict the performance of different modalities after integration. The experimental part of the classification accuracy experiments on four different datasets with complete and missing modalities and different network structures.
# datasets
  We use a set of four dataset with different tasks t evaluate the performance of learning.
## 1. UR-FUNNY dataset.
  The UR-FUNNY dataset is the first dataset for multimodal humor detection and was developed bye the Human-Computer Interaction Laboratory at the University of Rochester in collaboration with the Language Technologies Institute at Carnegie Mellon University. It contains text, audio and video modalities. The address of the link to the dataset is (https://roc-hci.com/current-projects/multimodal-humor-understanding/).
## 2. MVSA dataset.
  The dataset performs sentiment recognition tsak through text and image pairs.It consists of two separate datasets, the MVSA-Single dataset and the MVSA-Multi dataset. The former has only one annotation per graphic pair and contains 5129 graphic pairs. The latter has each graphic pair given by three annotators and contains 19,600 graphic pairs. The dataset is available at (https://mcrlab.net/research/mvsa-sentiment-analysis-on-multi-view-social-data/).
## 3. UPMC-Food-101 dataset.
  The dataset contains 101 food categories with 750 training image-text pairs and 250 test image-text pairs per category for categorization. The dataset is available at (https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/).
## 4. CMU-MOSEI dataset.
  The dataset is a text-audio-video dataset focusing on sentiment analysis and emotion recognition. The dataset is available at (http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/).
